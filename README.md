# QuestrionAI â€“ AI-Powered Document Q&A Platform (Built with RAG)
QuestrionAI is a **Retrieval-Augmented Generation (RAG)** powered full-stack application that lets you **upload PDFs and chat with them**. It extracts content, generates **OpenAI embeddings**, stores them in **Qdrant**, and retrieves the most relevant chunks to ground LLM answers.  

The app is containerized with **Docker** and deployed on **AWS EC2** (backend) and **Vercel** (frontend), with **Clerk** for authentication.

## ğŸŒ Live URLs

- **Frontend**: https://questrion.vercel.app  
- **Backend**: https://questrionai.ddnsfree.com  

âš ï¸ **Note:** The backend must be running for the application to function properly. It handles document ingestion, embeddings, vector search, and AI responses.

## ğŸš€ Features

- **RAG-based Q&A** â€“ Retrieval-Augmented Generation pipeline for grounded answers  
- **PDF Upload & Ingestion** â€“ Extracts text, cleans it, chunks into passages, and embeds with OpenAI  
- **Chat with Documents** â€“ Interactive Q&A over your uploaded content  
- **Vector Search (Qdrant)** â€“ Similarity search to find the most relevant chunks  
- **Conversation Memory** â€“ Maintains history and context across sessions  
- **Smart Suggestions** â€“ AI-generated follow-up questions for smoother interaction  
- **Authentication** â€“ Secure login/sign-up with Clerk  
- **Production Ready** â€“ Dockerized services, deployed via Docker Compose on AWS EC2, frontend on Vercel

## ğŸ› ï¸ Tech Stack

- **Frontend:** React + Vite, TypeScript  
- **Backend:** Python **FastAPI**, Uvicorn  
- **AI/RAG:** OpenAI Embeddings (`text-embedding-3-large`) + Retrieval pipeline  
- **Vector DB:** Qdrant  
- **Auth:** Clerk  
- **Infra:** Docker, Docker Compose, AWS EC2, Nginx reverse proxy  
- **Hosting:** Vercel (frontend), EC2 (backend)

## ğŸ“¦ How It Works (RAG Pipeline)

1. **Upload PDF** â†’ Extract + Clean â†’ Chunk into passages  
2. **Embed** â†’ Generate embeddings (OpenAI) â†’ Store in Qdrant  
3. **Query** â†’ Retrieve top-k relevant chunks â†’ Build context prompt  
4. **Answer** â†’ LLM generates grounded response using retrieved context  
